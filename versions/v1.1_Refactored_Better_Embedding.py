"""
è¶…ç®€å• RAG Demo - é€šä¹‰åƒé—®ç‰ˆæœ¬ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
åŠŸèƒ½ï¼šåŸºäºæ–‡æ¡£çš„é—®ç­”ç³»ç»Ÿ
ç‰ˆæœ¬ï¼šv1.2 - 2026.01.19 - åŒ…å«æ–‡ä»¶è¯»å–ã€ä¼˜åŒ–æç¤ºè¯ã€å¯¹è¯å†å²

ä½¿ç”¨å‰éœ€è¦å®‰è£…ï¼š
pip install dashscope chromadb
"""

import dashscope
import chromadb
from dashscope import TextEmbedding

# ==================== é…ç½®å¸¸é‡ ====================
# æç¤ºè¯æ¨¡æ¿é›†ä¸­ç®¡ç†
BASIC_PROMPT_TEMPLATE = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é—®ç­”åŠ©æ‰‹ã€‚è¯·ä»”ç»†é˜…è¯»ä¸‹é¢çš„æ–‡æ¡£å†…å®¹ï¼Œä»ä¸­æå–ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

æ–‡æ¡£å†…å®¹ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

å›ç­”è¦æ±‚ï¼š
1. å¦‚æœæ–‡æ¡£ä¸­æ˜ç¡®æåˆ°äº†ç­”æ¡ˆï¼Œè¯·ç›´æ¥å›ç­”
2. å¦‚æœæ–‡æ¡£ä¸­æœ‰ç›¸å…³ä¿¡æ¯ä½†ä¸å¤Ÿå®Œæ•´ï¼Œè¯·åŸºäºå·²æœ‰ä¿¡æ¯å›ç­”
3. åªæœ‰åœ¨æ–‡æ¡£å®Œå…¨æ²¡æœ‰ç›¸å…³ä¿¡æ¯æ—¶ï¼Œæ‰è¯´"æ–‡æ¡£ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯"

è¯·ç”¨ä¸­æ–‡ç®€æ´å›ç­”ï¼š"""

TEACHER_PROMPT_TEMPLATE = """ä½ æ˜¯ä¸€ä¸ªä¼˜ç§€çš„å­¦ä¹ åŠ©æ‰‹ï¼Œè¯·åŸºäºä¸‹é¢çš„çŸ¥è¯†åº“å†…å®¹ï¼Œç”¨è‡ªç„¶ã€æ˜“æ‡‚çš„æ–¹å¼å›ç­”é—®é¢˜ã€‚

ç›¸å…³èƒŒæ™¯çŸ¥è¯†ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·æŒ‰ç…§ä»¥ä¸‹è¦æ±‚å›ç­”ï¼š
1. é¦–å…ˆç†è§£æ–‡æ¡£ä¸­çš„æ ¸å¿ƒæ¦‚å¿µ
2. ç”¨ä½ è‡ªå·±çš„è¯è§£é‡Šï¼Œè€Œä¸æ˜¯ç›´æ¥å¤åˆ¶åŸæ–‡
3. å¦‚æœæ–‡æ¡£ä¸­æœ‰ä¾‹å­ï¼Œå¯ä»¥ç”¨è‡ªå·±çš„è¯é‡è¿°ä¾‹å­
4. ä¿æŒå›ç­”ç®€æ´æ˜äº†ï¼Œé€‚åˆå­¦ä¹ è€…ç†è§£
5. å¦‚æœæ–‡æ¡£ä¿¡æ¯ä¸è¶³ï¼Œå¯ä»¥åŸºäºå¸¸è¯†è¡¥å……ï¼Œä½†è¦æ³¨æ˜"åŸºäºä¸€èˆ¬çŸ¥è¯†"

è¯·å¼€å§‹å›ç­”ï¼š"""

# æ–‡ä»¶è·¯å¾„å¸¸é‡
KNOWLEDGE_FILE = "data/your_notes.txt"
VECTOR_DB_NAME = "my_docs"

# æ£€ç´¢å‚æ•°
TOP_K_RESULTS = 3
HISTORY_WINDOW_SIZE = 3  # å¯¹è¯å†å²çª—å£å¤§å°

# ==================== APIé…ç½® ====================
# è®¾ç½® API Key
dashscope.api_key = "sk-b6c13c57c648404b95bbffb80baa0133"


# ==================== Embeddingç±» ====================
class QwenEmbeddingFunction:
    """é€šä¹‰åƒé—® Embedding å‡½æ•°å°è£…"""

    def _get_embeddings(self, texts):
        """è°ƒç”¨é€šä¹‰åƒé—® API è·å– embeddings"""
        if isinstance(texts, str):
            texts = [texts]

        try:
            response = TextEmbedding.call(
                model=TextEmbedding.Models.text_embedding_v3,
                input=texts
            )

            print(f"API å“åº”çŠ¶æ€: {response.status_code}")
            if response.status_code != 200:
                print(f"API é”™è¯¯: {response.code} - {response.message}")
                raise Exception(f"Embedding API è°ƒç”¨å¤±è´¥: {response.message}")

            return [item['embedding'] for item in response.output['embeddings']]
        except Exception as e:
            print(f"âŒ Embedding è°ƒç”¨å‡ºé”™: {e}")
            raise

    def __call__(self, input):
        """Chroma ä¼šè°ƒç”¨è¿™ä¸ªæ–¹æ³•"""
        return self._get_embeddings(input)

    def embed_documents(self, texts):
        """å­˜å‚¨æ–‡æ¡£æ—¶è°ƒç”¨"""
        return self._get_embeddings(texts)

    def embed_query(self, input):
        """æŸ¥è¯¢æ—¶è°ƒç”¨ - æ³¨æ„ Chroma å¯èƒ½ä¼ å…¥åˆ—è¡¨"""
        if isinstance(input, list):
            if len(input) > 0:
                query_text = input[0]
            else:
                raise ValueError("input åˆ—è¡¨ä¸ºç©º")
        else:
            query_text = input

        if not isinstance(query_text, str):
            query_text = str(query_text)

        embeddings = self._get_embeddings([query_text])
        return embeddings


# ==================== æ–‡æ¡£å¤„ç†å‡½æ•° ====================
def load_documents_from_file(file_path=KNOWLEDGE_FILE):
    """ä»æ–‡ä»¶ä¸­è¯»å–æ–‡æ¡£å¹¶æŒ‰æ®µè½åˆ†å‰²"""
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    # æŒ‰æ®µè½åˆ†å‰²ï¼ˆæ¯ä¸ªæ®µè½æ˜¯ä¸€ä¸ªçŸ¥è¯†ç‚¹ï¼‰
    paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
    return paragraphs


# ==================== åˆå§‹åŒ–å‘é‡æ•°æ®åº“ ====================
def initialize_vector_database():
    """åˆå§‹åŒ–Chromaå‘é‡æ•°æ®åº“å¹¶åŠ è½½æ–‡æ¡£"""
    client = chromadb.Client()
    collection = client.create_collection(
        name=VECTOR_DB_NAME,
        embedding_function=QwenEmbeddingFunction()
    )

    documents = load_documents_from_file()
    for i, doc in enumerate(documents):
        collection.add(
            documents=[doc],
            ids=[f"doc_{i}"]
        )

    print(f"âœ… å·²åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“")
    return collection


# ==================== æ ¸å¿ƒRAGå‡½æ•° ====================
def retrieve_context(collection, question, top_k=TOP_K_RESULTS):
    """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
    results = collection.query(
        query_texts=[question],
        n_results=top_k
    )

    print(f"\nğŸ” æ£€ç´¢åˆ°çš„æ–‡æ¡£:")
    for i, doc in enumerate(results['documents'][0], 1):
        print(f"  {i}. {doc[:100]}...")  # åªæ‰“å°å‰100å­—ç¬¦
    print()

    context = "\n".join(results['documents'][0])
    return context


def ask_question(question: str, prompt_template=TEACHER_PROMPT_TEMPLATE) -> str:
    """æ ¸å¿ƒé—®ç­”å‡½æ•°"""
    # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
    context = retrieve_context(collection, question)

    # 2. æ„é€ æç¤ºè¯
    prompt = prompt_template.format(context=context, question=question)

    # 3. è°ƒç”¨é€šä¹‰åƒé—®ç”Ÿæˆç­”æ¡ˆ
    response = dashscope.Generation.call(
        model='qwen-plus',
        prompt=prompt,
        result_format='message'
    )

    if response.status_code == 200:
        return response.output.choices[0].message.content
    else:
        return f"è°ƒç”¨å¤±è´¥: {response.message}"


# ==================== å¯¹è¯å†å²ç®¡ç† ====================
conversation_history = []


def ask_question_with_history(question):
    """å¸¦å†å²ä¸Šä¸‹æ–‡çš„é—®ç­”"""
    answer = ask_question(question)

    # ä¿å­˜åˆ°å†å²
    conversation_history.append(f"Q: {question}")
    conversation_history.append(f"A: {answer}")

    # ä¿æŒå†å²çª—å£å¤§å°
    if len(conversation_history) > HISTORY_WINDOW_SIZE * 2:
        conversation_history.pop(0)
        conversation_history.pop(0)

    return answer


def get_recent_history(window_size=HISTORY_WINDOW_SIZE):
    """è·å–æœ€è¿‘çš„å¯¹è¯å†å²"""
    return conversation_history[-(window_size * 2):] if conversation_history else []


# ==================== æµ‹è¯•å‡½æ•° ====================
def run_test_questions(questions, use_history=False):
    """è¿è¡Œæµ‹è¯•é—®é¢˜é›†"""
    print("\n" + "=" * 60)
    print("ğŸ¤– RAG é—®ç­”ç³»ç»Ÿæµ‹è¯•å¼€å§‹")
    print("=" * 60 + "\n")

    for i, q in enumerate(questions, 1):
        print(f"\n[{i}/{len(questions)}] é—®é¢˜: {q}")

        if use_history:
            answer = ask_question_with_history(q)
        else:
            answer = ask_question(q)

        print(f"ğŸ’¡ ç­”æ¡ˆ: {answer}")
        print("-" * 50)

    if use_history:
        print(f"\nğŸ“ å¯¹è¯å†å²: {get_recent_history()}")


# ==================== ä¸»ç¨‹åº ====================
if __name__ == "__main__":
    # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
    collection = initialize_vector_database()

    # æµ‹è¯•é—®é¢˜é›†
    test_questions = [
        "ä»€ä¹ˆæ˜¯è¿‡æ‹Ÿåˆï¼Ÿå¦‚ä½•è§£å†³ï¼Ÿ",
        "æœºå™¨å­¦ä¹ çš„è¦ç´ æœ‰å“ªäº›ï¼Ÿ",
        "CNNå’ŒRNNåˆ†åˆ«ç”¨äºä»€ä¹ˆï¼Ÿ",
        "ä»€ä¹ˆæ˜¯Embeddingï¼Ÿ",
        "RAGæœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ",
        "æ¿€æ´»å‡½æ•°çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ"
    ]

    # è¿è¡Œæµ‹è¯•ï¼ˆä¸å¸¦å†å²ï¼‰
    run_test_questions(test_questions, use_history=False)

    # æ¸…ç©ºå†å²ï¼Œé‡æ–°æµ‹è¯•å¸¦å†å²çš„ç‰ˆæœ¬
    conversation_history.clear()
    print("\n\n" + "=" * 60)
    print("ğŸ”„ å¼€å§‹å¸¦å†å²ä¸Šä¸‹æ–‡çš„æµ‹è¯•")
    print("=" * 60 + "\n")

    run_test_questions(test_questions[:3], use_history=True)  # åªæµ‹è¯•å‰3ä¸ª

    print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼")